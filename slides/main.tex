%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[aspectratio=169,xcolor=dvipsnames]{beamer}
\usetheme{SimplePlus}

\usepackage{hyperref}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath}  % Added for math formatting
\usepackage{amssymb}  % Added for math symbols
\usepackage{algorithm} % Added for algorithms
\usepackage{algpseudocode} % Added for algorithms
\usepackage{hyperref} % Added for videos
\usepackage{multimedia}
\setbeamertemplate{footline}[frame number]

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Reinforcement Learning}

%\subtitle{}

% The presenter
\author{Alessio Russo}

\institute
{
    Department of Mathematical, Physical and Computer Sciences\\
    University of Parma
}
\date{}

%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{document}

% Slide 1: Title
\begin{frame}[plain, noframenumbering]
    \titlepage
\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: BACKGROUND
%----------------------------------------------------------------------------------------

\begin{frame}{What is Reinforcement Learning?}

    \textbf{Core Principle}
    \begin{itemize}
        \item No pre-defined labels or explicit teacher.
        \item Agent learns by \textit{interacting} with dynamic environment.
        \item Objective: maximize cumulative reward signal.
        \item Uses deep neural networks, SGD, backpropagation with different data flow.
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Key Challenges}
    \begin{enumerate}
        \item \textbf{Non-IID Data:} Observations depend on agent's policy; poor policy $\Rightarrow$ poor coverage.
        \item \textbf{Exploration vs.\ Exploitation:} Balance trying new actions vs.\ using known rewards.
        \item \textbf{Credit Assignment:} Actions yield rewards many steps later (e.g., chess moves).
    \end{enumerate}
\end{frame}

%------------------------------------------------

\begin{frame}{Agent-Environment Interaction}
    RL architecture rests on continuous interaction between agent and environment.
    
    \vspace{0.3cm}
    
    \textbf{Entities}
    \begin{itemize}
        \item \textbf{Agent:} Decision maker.
        \item \textbf{Environment:} Physical or virtual world.
    \end{itemize}
    
    \vspace{0.2cm}
    
    \textbf{Communication Channels}
    \begin{itemize}
        \item \textbf{Action ($a$):} Agent moves (discrete or continuous).
        \item \textbf{State ($s$):} Internal environment configuration.
        \item \textbf{Reward ($r$):} Scalar feedback signal.
    \end{itemize}
    
    \vspace{0.3cm}
    
    \small{\textit{Note:} An \textbf{observation} is a partial perception of the full state.}
\end{frame}

%------------------------------------------------

\begin{frame}{Markov Process and Markov Reward Process}
    A process is \textbf{Markovian} if the future depends only on the current state, not the history.
    \vspace{-0.1cm}
    $$P(s_{t+1} | s_t) = P(s_{t+1} | s_1, s_2, \dots, s_t)$$

    The dynamics are defined by a \textbf{Transition Matrix} $T$, where cell $(i,j)$ represents the probability of moving from state $i$ to state $j$: $T_{i,j} = P(S_{t+1} = j | S_t = i)$

    \vspace{0.2cm}
    An MRP extends the MP by associating a scalar reward with states or transitions. In particular:
    
    \begin{itemize}
        \item \textbf{Discount Factor ($\gamma$):} Determines present value of future rewards ($\gamma = 0$ myopic, $\gamma \to 1$ far-sighted).
        \item \textbf{State Value Function $V(s)$:} The expected return starting from state $s$: $V(s) = \mathbb{E}[G_t | S_t = s]$
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{Markov Decision Process (MDP)}
    The MDP introduces \textbf{Actions} ($A$), allowing the agent to influence the dynamics.
    
    \begin{itemize}
        \item The transition matrix becomes a 3D tensor $|S| \times |S| \times |A|$.
        \item Probability of transition depends on the action taken:
    \end{itemize}
    
    \begin{equation}
        P(s' | s, a) = P(S_{t+1} = s' | S_t = s, A_t = a)
    \end{equation}
    
    \begin{itemize}
        \item The Reward function also becomes action-dependent:
    \end{itemize}
    
    \begin{equation}
        R(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]
    \end{equation}
\end{frame}

%------------------------------------------------

\begin{frame}{The Policy ($\pi$)}
    The \textbf{Policy} is the agent's strategy or "brain". It defines the behavior in any given state.
    
    \begin{block}{Stochastic Policy Definition}
        It is a probability distribution over actions given states:
        $$\pi(a|s) = P[A_t = a | S_t = s]$$
    \end{block}
    
    \begin{itemize}
        \item \textbf{Stochasticity:} Essential for exploration.
        \item \textbf{Reduction:} If a policy is fixed, an MDP reduces to an MRP, as the transition probabilities become averaged over the policy's choices.
    \end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Gymnasium: The RL Environment Standard}
    \textbf{Gymnasium} provides a unified API for RL environments via the \texttt{Env} class.
    
    \vspace{0.3cm}
    
    \textbf{Core Interface}
    \begin{itemize}
        \item \textbf{Action Space:} Discrete or Continuous
        \item \textbf{Observation Space:} Shape, bounds
        \item \textbf{\texttt{reset()}} $\to s_0$
        \item \textbf{\texttt{step(a)}} $\to$ (observation, reward, terminated, truncated, info)
    \end{itemize}
    
    \vspace{0.3cm}
    
    \textbf{Space Types}
    \begin{itemize}
        \item \textbf{Discrete:} Finite actions (e.g., \texttt{Discrete(2)})
        \item \textbf{Box:} Continuous $\mathbb{R}^n$ (e.g., images, sensors)
        \item \textbf{Tuple/Dict:} Complex structures
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: MCTS on CartPole-v1
%----------------------------------------------------------------------------------------

\begin{frame}{MCTS applied to CartPole-v1}
    \textbf{Problem:} Balance pole on moving cart. \textbf{Reward:} $+1$ per step upright.
    
    \vspace{0.2cm}
    
    \textbf{Environment Specifications}
    \begin{itemize}
        \item \textbf{Obs.\ Space (Box 4):} [Cart Pos, Cart Vel, Pole Angle, Pole Ang.\ Vel]
        \item \textbf{Action Space (Discrete 2):} \{0: Push Left, 1: Push Right\}
    \end{itemize}
    
    \vspace{0.3cm}

    \textbf{Core Algorithm Flow}
    \begin{enumerate}
        \item \textbf{Selection:} Traverse tree via UCB1 rule: $\bar{Q}(s,a) + 1.41\sqrt{\frac{\ln N(s)}{N(s,a)}}$
        \item \textbf{Expansion:} Pop untried action, step simulator, create child node.
        \item \textbf{Simulation:} Run random rollout (max 30 steps) to accumulate return $G$.
        \item \textbf{Backpropagation:} Update $N \leftarrow N+1$, $W \leftarrow W+G$ along path to root.
    \end{enumerate}
    
\end{frame}

%------------------------------------------------

\begin{frame}{MCTS applied to CartPole-v1}

    \textbf{State Synchronization (Critical)}
    \begin{itemize}
        \item Before each phase: reset environment and force internal state to root.
        \item Replay action sequence from root to current node.
        \item Ensures simulator state matches tree node semantics.
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Online Replanning}
    \begin{itemize}
        \item At each real timestep: run $\sim 40$ MCTS iterations from current observation.
        \item Return root child with \textbf{highest visit count} (most robust under exploration noise).
        \item Execute action in true environment; repeat.
    \end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{MCTS applied to CartPole-v1}
    \centering
    \movie[externalviewer]{\includegraphics[width=\textheight, keepaspectratio]{images/cart_pole_mcts.png}}{videos/cart_pole_mcts.mp4}
\end{frame}


%----------------------------------------------------------------------------------------
%    SECTION: CEM on CartPole-v1
%----------------------------------------------------------------------------------------

\begin{frame}{Cross-Entropy Method (CEM)}

    \textbf{Core Idea}
    \begin{itemize}
        \item Maintain a distribution over policy parameters.
        \item Sample policies from this distribution; evaluate on trajectories.
        \item Identify \textit{elite} samples (top $p$-th percentile by reward).
        \item Refit distribution to elite samples, concentrating mass on better regions.
    \end{itemize}
    
    \vspace{0.2cm}
    
    \textbf{RL Application}
    \begin{itemize}
        \item Policy parameterized by neural network weights $\theta$.
        \item Batch generation: sample $\theta$ from prior, rollout episodes, measure rewards.
        \item Filter batch by percentile; supervise model to predict elite actions via cross-entropy loss.
        \item Repeat: distribution implicitly shifts toward high-reward regions.
    \end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{CEM applied to CartPole-v1}

    \textbf{Model Architecture}
    A mlp with $[4 \to 128 \to 2]$ (observation to actions).

    \textbf{Training Loop}
    \begin{enumerate}
        \item \textbf{Batch Generation:} Generate $B$ episodes using current policy $\pi_\theta$ (via softmax).
        \item \textbf{Elite Filtering:} Compute reward percentile; keep steps from episodes exceeding threshold.
        \item \textbf{Supervised Learning:} Train model via cross-entropy loss to predict elite actions:
        $$\mathcal{L} = -\sum_{(s,a) \in \text{elite}} \log \pi_\theta(a|s)$$
    \end{enumerate}

    \vspace{0.3cm}

    For final evaluation, argmax selects the best action

\end{frame}

%------------------------------------------------

\begin{frame}{CEM applied to CartPole-v1}
    \centering
    \movie[externalviewer]{\includegraphics[width=\textheight, keepaspectratio]{images/cart_pole_cem.png}}{videos/cart_pole_cem.mp4}
\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: CEM on CarRacing-v3
%----------------------------------------------------------------------------------------

\begin{frame}{CEM applied to CarRacing-v3}
    \textbf{Problem:} Control a race car from a top-down pixel perspective.
    
    \vspace{0.3cm}
    
    \textbf{Environment Specifications}
    \begin{itemize}
        \item \textbf{Observation Space:} RGB Image ($96 \times 96 \times 3$).
        \begin{itemize}
            \item \textit{Preprocessing:} Resized to $224 \times 224$, Normalized for ResNet.
        \end{itemize}
        \item \textbf{Action Space (Discrete 5):}
        \begin{enumerate}
            \item Do Nothing
            \item Steer Left
            \item Steer Right
            \item Gas
            \item Brake
        \end{enumerate}
        \item \textbf{Reward Signal:}
        \begin{itemize}
            \item Positive reward for visiting new track tiles.
            \item Small penalty ($-0.1$) per frame to encourage speed.
        \end{itemize}
    \end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{CEM applied to CarRacing-v3}
    \textbf{Model Architecture}
    \begin{itemize}
        \item \textbf{Backbone:} \texttt{ResNet18} (Pretrained on ImageNet).
        \item \textbf{Head:} Linear layer mapping features $\to$ 5 action logits.
    \end{itemize}
    
    \vspace{0.3cm}
    
    \textbf{Action Masking.} The agent is guided through early exploration by masking suboptimal actions:
    \begin{itemize}
        \item \textbf{Phase 1} Mask \{0, 1, 2, 4\}.
        \begin{itemize}
            \item \textit{Effect:} Forces \textbf{Gas} (Action 3) to prevent idling.
        \end{itemize}
        \item \textbf{Phase 2:} Full action space enabled.
    \end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{CEM applied to CarRacing-v3}
    \centering
    \movie[externalviewer]{\includegraphics[width=\textheight, keepaspectratio]{images/car_racing_cem.png}}{videos/car_racing_cem.mp4}
\end{frame}

%------------------------------------------------

\begin{frame}[plain, noframenumbering]
    \centering

    \Large{\centerline{\textbf{Thank you for your attention!}}}
    \vspace{1.5cm}
    \footnotesize
    \textit{Alessio Russo}\\
    \textit{University of Parma}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}
